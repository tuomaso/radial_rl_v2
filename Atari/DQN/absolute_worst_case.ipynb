{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import read_config\n",
    "from model import DuelingCnnDQN\n",
    "from environment import make_atari, wrap_deepmind, wrap_pytorch, make_atari_cart\n",
    "from ibp import network_bounds, subsequent_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class ArgHelper(object):\n",
    "    def __init__(self, env, gpu_id, skip_rate, max_episode_length, load_path, env_config):\n",
    "        self.env = env\n",
    "        self.gpu_id = gpu_id\n",
    "        self.skip_rate = skip_rate\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.load_path = load_path\n",
    "        self.env_config = env_config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgHelper(env = 'FreewayNoFrameskip-v4',\n",
    "    gpu_id = 0,\n",
    "    skip_rate = 4,\n",
    "    max_episode_length = 10000,\n",
    "    load_path = 'trained_models/Freeway_robust.pt',#'trained_models/Pong_robust.pt',\n",
    "    env_config = 'config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env():\n",
    "    setup_json = read_config(args.env_config)\n",
    "    env_conf = setup_json[\"Default\"]\n",
    "    for i in setup_json.keys():\n",
    "        if i in args.env:\n",
    "            env_conf = setup_json[i]\n",
    "    #env = atari_env(args.env, env_conf, args)\n",
    "\n",
    "    if \"NoFrameskip\" not in args.env:\n",
    "        env = make_atari_cart(args.env)\n",
    "    else:\n",
    "        env = make_atari(args.env)\n",
    "        env = wrap_deepmind(env, central_crop=True, clip_rewards=False, episode_life=False, **env_conf)\n",
    "        env = wrap_pytorch(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gpu_id < 0:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = DuelingCnnDQN(env.observation_space.shape[0], env.action_space)\n",
    "new_dict = torch.load(args.load_path, map_location=device)\n",
    "try:\n",
    "    current_model.load_state_dict(new_dict['model_state_dict'])\n",
    "except(RuntimeError):\n",
    "    current_model.load_state_dict(new_dict)\n",
    "current_model = current_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(curr_model, env, epsilon, state, steps):\n",
    "    next_envs = []\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    value, advs = curr_model.forward(state)\n",
    "    output = value + advs\n",
    "    #print(output)\n",
    "    action = torch.argmax(output, dim=1)\n",
    "\n",
    "    upper, lower = network_bounds(curr_model.cnn, state, epsilon)\n",
    "    upper, lower = subsequent_bounds(curr_model.advantage, upper, lower)\n",
    "    upper += value\n",
    "    lower += value\n",
    "    impossible = upper < torch.max(lower, dim=1)[0]\n",
    "    \n",
    "    snapshot = env.ale.cloneState()\n",
    "    for i in range(impossible.shape[1]):\n",
    "        if (not impossible[0, i]):\n",
    "            next_state, reward, done, _ = env.step(i)\n",
    "            #Won the game, no need to check future states\n",
    "            if reward > 1e-5:\n",
    "                env.ale.restoreState(snapshot)\n",
    "                continue\n",
    "            else:\n",
    "                if steps<=1 or reward < -1e-5:\n",
    "                    env.ale.restoreState(snapshot)\n",
    "                    return -1\n",
    "                else:\n",
    "                    next_envs.append((env.ale.cloneState(), next_state, steps-1))\n",
    "                    env.ale.restoreState(snapshot)\n",
    "    #base case, can't reach the goal with 0 steps\n",
    "    return next_envs\n",
    "\n",
    "def get_greedy_worst_case(curr_model, env, epsilon, state, max_steps):\n",
    "    orig_env = env.ale.cloneState()\n",
    "    for _ in range(max_steps):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        value, advs = curr_model.forward(state)\n",
    "        output = value + advs\n",
    "        action = torch.argmax(output, dim=1)\n",
    "\n",
    "        upper, lower = network_bounds(curr_model.cnn, state, epsilon)\n",
    "        upper, lower = subsequent_bounds(curr_model.advantage, upper, lower)\n",
    "        upper += value\n",
    "        lower += value\n",
    "        impossible = upper < torch.max(lower, dim=1)[0]\n",
    "        worst_case_action = torch.argmin(output+1e6*impossible, dim=1)\n",
    "        next_state, reward, done, _ = env.step(worst_case_action[0])\n",
    "        \n",
    "        if reward > 1e-5:\n",
    "            env.ale.restoreState(orig_env)\n",
    "            #print(\"Greedy worst case reward: {}\".format(reward))\n",
    "            return 1\n",
    "        elif reward < -1e-5:\n",
    "            env.ale.restoreState(orig_env)\n",
    "            #print(\"Greedy worst case reward: {}\".format(reward))\n",
    "            return -1\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    env.ale.restoreState(orig_env)\n",
    "    return -1\n",
    "    \n",
    "\n",
    "def get_action_cert_rate(curr_model, env, epsilon, state, max_steps):\n",
    "    certified = 0\n",
    "    total = 0\n",
    "    orig_env = env.ale.cloneState()\n",
    "    for _ in range(max_steps):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        value, advs = curr_model.forward(state)\n",
    "        output = value + advs\n",
    "        #print(output)\n",
    "        action = torch.argmax(output, dim=1)\n",
    "\n",
    "        upper, lower = network_bounds(curr_model.cnn, state, epsilon)\n",
    "        upper, lower = subsequent_bounds(curr_model.advantage, upper, lower)\n",
    "        upper += value\n",
    "        lower += value\n",
    "        \n",
    "        upper[:, action] = -1e10    \n",
    "        max_other = torch.max(upper, dim=1)[0]\n",
    "        if lower[:, action] > max_other:\n",
    "            certified += 1\n",
    "        total += 1\n",
    "        \n",
    "        action = torch.argmax(output, dim=1)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action[0])\n",
    "        \n",
    "        if abs(reward) > 1e-5:\n",
    "            env.ale.restoreState(orig_env)\n",
    "            return certified/total\n",
    "        \n",
    "        else:\n",
    "            state = next_state\n",
    "    env.ale.restoreState(orig_env)\n",
    "    return certified/total\n",
    "\n",
    "def worst_case_reward(curr_model, env, epsilon, max_steps):\n",
    "    envs_to_check = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    pos_rewards = 0\n",
    "    neg_rewards = 0\n",
    "    paths = 1\n",
    "    with torch.no_grad():\n",
    "        envs_to_check.append((env.ale.cloneState(), state, max_steps))\n",
    "        \n",
    "        greedy_reward = get_greedy_worst_case(curr_model, env, epsilon, state, max_steps)\n",
    "        acr = get_action_cert_rate(curr_model, env, epsilon, state, max_steps)\n",
    "        \n",
    "        while len(envs_to_check)>0:\n",
    "            \n",
    "            snapshot, state, steps_remaining = envs_to_check.pop(-1)\n",
    "            env.ale.restoreState(snapshot)\n",
    "            out = get_next(curr_model, env, epsilon, state, steps_remaining) \n",
    "            if out == -1:\n",
    "                return -1, greedy_reward, paths, acr\n",
    "            else:\n",
    "                envs_to_check.extend(out)\n",
    "                paths += max(0,len(out)-1)\n",
    "            #if (len(next_envs)-1) > 0 and paths%500==0:\n",
    "                \n",
    "            if paths > 2000:\n",
    "                print(paths, len(envs_to_check))\n",
    "                return 0, greedy_reward, paths, acr\n",
    "        return 1, greedy_reward, paths, acr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed, env):\n",
    "    #set seeds for reproducible results\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    env.action_space.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "verified_rewards = []\n",
    "greedy_rewards = []\n",
    "acrs = []\n",
    "epsilons = np.array([1, 1.2, 1.3, 1.4, 1.5])/255#np.array([1, 1.2, 1.3, 1.4, 1.5])/255\n",
    "#np.array([0.1, 0.3, 1, 1.1, 1.15, 1.2, 1.3, 3, 8])/255\n",
    "\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    verified = []\n",
    "    greedy = []\n",
    "    acr = []\n",
    "    print('Epsilon: {}'.format(epsilon))\n",
    "    for j in range(20):\n",
    "        env = create_env()\n",
    "        set_seed(j,env)\n",
    "        reward, greedy_reward, paths, acr_res = worst_case_reward(current_model, env, epsilon, max_steps=80)\n",
    "        print('Greedy: {}, Absolute worst case reward:{}, paths checked:{}, action cert rate:{:.4f}'.format(greedy_reward,\n",
    "                                                                                                     reward, paths, acr_res))\n",
    "        if reward != 0:\n",
    "            verified.append(reward)\n",
    "            greedy.append(greedy_reward)\n",
    "            acr.append(acr_res)\n",
    "    verified_rewards.append(verified)\n",
    "    greedy_rewards.append(greedy)\n",
    "    acrs.append(acr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "greed = [np.mean(i) for i in greedy_rewards]\n",
    "ver = [np.mean(i) for i in verified_rewards]\n",
    "acr_ = [np.mean(i) for i in acrs]\n",
    "\n",
    "plt.plot(epsilons*255, greed, marker='o', label='Greedy worst case reward')\n",
    "plt.plot(epsilons*255, ver, marker='.', label='Absolute worst case reward')\n",
    "plt.plot(epsilons*255, np.array(acr_)*2-1, marker='s', label='(Action certification rate)*2-1')\n",
    "\n",
    "plt.legend(bbox_to_anchor=[1,1.5])\n",
    "plt.xlabel('epsilon*255')\n",
    "plt.ylabel('Average result')\n",
    "#plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'size'   : 14}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "x = epsilons*255 # the label locations\n",
    "width = 0.01  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects1 = ax.bar(x - width - 0.0015, (np.array(greed)+1)/2, width, label='GWC', color=np.array((255,153,51))/255)\n",
    "rects2 = ax.bar(x, (np.array(ver)+1)/2, width, label='AWC', color=np.array((30,144,255))/255)\n",
    "rects3 = ax.bar(x + width + 0.002, acr_, width, label='ACR', color=np.array((40,164,40))/255)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Average result')\n",
    "ax.set_xlabel('$\\epsilon*255$')\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0.95,1.55)\n",
    "#ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "\n",
    "def label(rect, color=(0,0,0), offset=(0,3)):\n",
    "    height = rect.get_height()\n",
    "    ax.annotate('{:.2f}'.format(rect.get_height()),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, rect.get_height()),\n",
    "                        xytext=offset,  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\", color=color,\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "# label(rects1[2], np.array((255,153,51))/255 ,offset=(-20,-5))\n",
    "# label(rects1[3], np.array((255,153,51))/255 ,offset=(-20,-20))\n",
    "# label(rects1[4], offset=(-7,3))\n",
    "\n",
    "# label(rects2[2], np.array((30,144,255))/255 ,offset=(-26,-10))\n",
    "# label(rects2[3], np.array((30,144,255))/255 ,offset=(-26,-10))\n",
    "\n",
    "# label(rects3[2], np.array((40,164,40))/255, offset=(20,-15))\n",
    "# label(rects3[3], np.array((40,164,40))/255, offset=(20,-20))\n",
    "# label(rects3[4], np.array((40,164,40))/255, (0,3))\n",
    "label(rects1[2], np.array((255,153,51))/255 ,offset=(-20,-17))\n",
    "label(rects1[3], np.array((255,153,51))/255 ,offset=(-19,-14))\n",
    "label(rects1[4], np.array((255,153,51))/255, offset=(-7,16))\n",
    "\n",
    "label(rects2[2], np.array((30,144,255))/255 ,offset=(-26,-22))\n",
    "label(rects2[3], np.array((30,144,255))/255 ,offset=(-26,-5))\n",
    "label(rects2[4], np.array((30,144,255))/255, offset=(-7,3))\n",
    "\n",
    "label(rects3[2], np.array((40,164,40))/255, offset=(-33,-14))\n",
    "label(rects3[3], np.array((40,164,40))/255, offset=(-33,-25))\n",
    "label(rects3[4], np.array((40,164,40))/255, (-20,-140))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(epsilons)):\n",
    "    print(epsilons[i]*255, len(greedy_rewards[i]))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_with_eps(eps_index):\n",
    "    pos_acrs = []\n",
    "    pos_gwcs = []\n",
    "    neg_acrs = []\n",
    "    neg_gwcs = []\n",
    "\n",
    "    for i in range(len(verified_rewards[eps_index])):\n",
    "        if verified_rewards[eps_index][i]==1:\n",
    "            pos_acrs.append(acrs[eps_index][i])\n",
    "            pos_gwcs.append(greedy_rewards[eps_index][i])\n",
    "            \n",
    "        elif verified_rewards[eps_index][i]==-1:\n",
    "            neg_acrs.append(acrs[eps_index][i])\n",
    "            neg_gwcs.append(greedy_rewards[eps_index][i])\n",
    "        \n",
    "    print('Epsilon: {}/255'.format(epsilons[eps_index]*255))\n",
    "    print('Average total AWC:{} GWC:{} ACR:{}'.format(np.mean(verified_rewards[eps_index]), \n",
    "                                                      np.mean(greedy_rewards[eps_index]), np.mean(acrs[eps_index])))\n",
    "    print('Positive AWC: {}/20'.format(len(pos_acrs)))\n",
    "    print('Average pos GWC:{} ACR:{}'.format(np.mean(pos_gwcs), np.mean(pos_acrs)))\n",
    "    print('Negative AWC: {}/20'.format(len(neg_acrs)))\n",
    "    print('Average neg GWC:{} ACR:{}'.format(np.mean(neg_gwcs), np.mean(neg_acrs)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(epsilons)):\n",
    "    result_with_eps(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
